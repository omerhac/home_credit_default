{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport gc\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \npd.set_option('use_inf_as_na', True) # treating infs!!!!!\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"I = [[8, 6, 2, 7], [6, 2, 4, 1], [5, 8, 5, 2], [3, 0, 3, 2]]\nK = [[4, 3], [7, 2]]\nS = [[2 ,7], [3, 4]]\nm = 4\nk = 2\n\nfrom scipy.signal import convolve2d\nprint(convolve2d(I,K, mode='valid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_encode_categorical(df, target_col):\n    \"\"\"Mean encode categorical features with epanding mean. nans will be replaced with mean of column. df: pd dataframe, target_col: target column to get values from\"\"\"\n    \n    df_with_mean = df.copy()\n    for column in df.columns:\n        if (df[column].nunique() <= 60) & (column != target_col): # only features with less than 60 unique values will be encoded\n            gb_col = df.groupby([column]) # groupby categorical feature\n            cumsum = gb_col[target_col].cumsum() - df[target_col] # sum target var uptill now (without this row)\n            df_with_mean[column] = cumsum / gb_col.cumcount() # mean encode feature\n            df_with_mean[column].fillna(df_with_mean[column].mean(), inplace=True) # fill nans with means\n            \n            \n    return df_with_mean\n\ndef test_mean_encode_categorical(df_test, df_train, target_col):\n    \"\"\"Mean encode categorical features with epanding mean. nans will be replaced with mean of column. df: pd dataframe, target_col: target column to get values from\"\"\"\n    \n    df_with_mean = df_test.copy()\n    for column in df_test.columns:\n        if (df_test[column].nunique() <= 60) & (column != target_col): # only features with less than 60 unique values will be encoded\n            gb_col = df_train.groupby([column]) # groupby categorical feature\n            means = gb_col[target_col].mean()\n            df_with_mean[column] = df_with_mean[column].map(means)\n            df_with_mean[column].fillna(df_with_mean[column].mean(), inplace=True) \n            \n    return df_with_mean\n\ndef print_outliers(df):\n    \"Print all numeric columns in boxplots. allows to see outliers. Dependencies: matplotlib.pyplot as plt, seaborn as sns\"\n    \n    reduce_df = df.select_dtypes(np.number)\n    num_plots = len(reduce_df.columns)\n    num_rows = num_plots / 4 + 1\n    plt.figure(figsize=(20,10 * num_rows)) # define the figure\n    \n    for i in range (num_plots): # print each numric col\n        plt.subplot(num_rows, 4, i + 1)\n        sns.boxplot(reduce_df[reduce_df.columns[i]])\n    \n    plt.show()\n\ndef print_null_places(df):\n    \"\"\"Print two graphs that shows how nans are placed across the data\"\"\"\n    \n    plt.figure(figsize=(20,10))\n    plt.subplot(1,2,1)\n    plt.plot(df.isnull().sum(axis=0)) # columnwise nulls\n    plt.title(\"columnwise nulls\")\n\n    plt.subplot(1,2,2)\n    plt.plot(df.isnull().sum(axis=1), ) # rowwise nulls\n    plt.title(\"rowwise nulls\")\n    \n    plt.show()\n\ndef replace_numeric_columns_nulls(df):\n    \"\"\"Replaces dataframe numeric column nulls with columns means / 0 if there is no mean\"\"\"\n    \n    numeric_cols = df.select_dtypes(np.number).columns\n    filled_df = df.copy()\n    for col in numeric_cols:\n        filled_df[col] = filled_df[col].fillna(filled_df[col].mean())\n    filled_df = filled_df.fillna(0)\n    return filled_df\n\ndef get_rows_above_threshold(df, column, threshold):\n    \"\"\"Gets rows from a dataframe which columns values are bigger than threshold\"\"\"\n    \n    return df.loc[df[column] > threshold, column]\n\ndef get_correlated_features(df, feature, threshold_corr):\n    \"\"\"Gets features with pearson correlation greater than threshold. args --> df: pd dataframe, feature: feature name to get correlated features for, threshold_corr: correlation threshold\"\"\"\n    \n    corr_feats = []\n    for feat in df.columns:\n        if (abs(df[feature].corr(df[feat])) > threshold_corr) & (feat != feature):\n            corr_feats.append(feat)\n    \n    return corr_feats\n\ndef get_correlation_graph(df, threshold):\n    \"\"\"Creats an undirectd graph of feature correlation. If two features correlation > threshold they will be connected with an edge\"\"\"\n    \n    corr_graph = Graph()\n    \n    for feat in df.columns:\n        if not corr_graph.has_node(feat):\n            corr_feats = get_correlated_features(df, feat, threshold)\n            for corr_feat in corr_feats:\n                corr_graph.add_edge((feat, corr_feat))\n    \n    return corr_graph\n\nclass Graph():\n    \"\"\"Undireced graph class\"\"\"\n    \n    def __init__(self):\n        self._dict = {}\n    \n    def add_node(self, node):\n        if node not in self._dict:\n            self._dict[node] = set()\n            \n    def add_edge(self, edge):\n        \"\"\"Adds an edge. If node didn't exist in the graph, adds it.\"\"\"\n        \n        (node1, node2) = edge\n        if node1 not in self._dict:\n            self._dict[node1] = set([node2])\n        else:\n            self._dict[node1].add(node2)\n        if node2 not in self._dict:\n            self._dict[node2] = set([node1])\n        else:\n            self._dict[node2].add(node1)\n    \n    def has_edge(self, edge):\n        (node1, node2) = edge\n        return (node2 in self._dict[node1])\n    \n    def has_node(self, node):\n        return node in self._dict\n    \n    def get_edges(self, node):\n        if self.has_node(node):\n            return self._dict[node]\n        else:\n            return None\n    \n    def print(self):\n        print(self._dict)\n    \ndef get_uncorrolated_features(df, threshold):\n    \"\"\"Gets all the features whic correlation is smaller than threshold.\n    Basiclly it returns [feature if feature_correlation_with_all_other_features < threshold]. \n    All features have to be numerical of encoded\"\"\"\n    \n    feats = list(df.columns)\n    corr_graph = get_correlation_graph(df, threshold)\n    un_corr_feats = []\n    \n    for feat in feats:\n        un_corr_feats.append(feat)\n        corr_feats = corr_graph.get_edges(feat)\n        if(corr_feats):\n            for to_del in corr_feats:\n                if to_del in feats:\n                    feats.remove(to_del)\n    \n    return un_corr_feats\n  \n    \ndef lgbm_eval(df,target_col, n_splits=1, test_size=0.20, verbose=False, get_model=False, n_rounds=200\n              , lgb_params={\n               'feature_fraction': 0.75,\n               'metric': 'auc',\n               'nthread':4, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.01, \n               'objective': 'binary', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':0,\n               'early_stopping_rounds': 100\n    }):\n    \n    \"\"\"Evaluates default parameters lgbm model's auc, uses stratified shuffle split. args --> df: pd dataframe, n_splits: int number of data splits,\n    test_size: each splits test data precentage, verbose: bool verbosity\"\"\" \n    \n    import lightgbm as lgb\n    from sklearn.model_selection import StratifiedShuffleSplit\n    from sklearn.metrics import roc_auc_score\n    \n    sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size)\n    X_train = df.drop([target_col], axis=1)\n    Y_train = df[target_col]\n    \n    loss = 0\n    \n    for train_indices, test_indices in sss.split(X_train,Y_train):\n        X_train_enc = mean_encode_categorical(df.iloc[train_indices], target_col).drop([target_col], axis=1) # mean encode train set\n        X_test_enc = test_mean_encode_categorical(df.iloc[test_indices].drop([target_col], axis=1), df.iloc[train_indices], target_col) # mean encode test set\n        model = lgb.train(lgb_params, lgb.Dataset(X_train_enc, label=Y_train.iloc[train_indices]), n_rounds, valid_sets=lgb.Dataset(X_test_enc, Y_train.iloc[test_indices]),\n                          verbose_eval=verbose) # train model\n        loss += roc_auc_score(Y_train.iloc[test_indices], model.predict(X_test_enc)) # accumulate loss\n        \n         # free up spaxe\n        del X_train_enc, X_test_enc\n        gc.collect()\n        \n    if get_model:\n        return (loss / sss.get_n_splits(), model) # avarage loss, model\n    else:\n        return loss / sss.get_n_splits() # loss\n    \n\ndef print_feature_correlation(df):\n    \"\"\"Print feature correlation\"\"\"\n    \n    plt.figure(figsize=(30,20))\n    sns.heatmap(df.corr())\n    plt.show()\n    \n\ndef normalize_dataframe(df):\n    \"\"\"Normalize dateframe numeric columns. Will not normalize binary columns. Dataframe has to be numeric\"\"\" \n    \n    df_norm = df.copy()\n    for column in df.columns:\n        if (list(df[column].unique()) != [0,1]) & (list(df[column].unique()) != [1,0]): # column in not part of one-hot-vector\n            c_range = df[column].max() - df[column].min() # range of column\n            df_norm[column] = (df[column] - df[column].mean()) / c_range\n    \n    return df_norm\n\n\ndef explore_target(df, target_column):\n    \"\"\"Print target balance and distirbution\"\"\"\n    \n    plt.figure(figsize=(30,60))\n    # target balnce\n    plt.subplot(2,1,1)\n    plt.hist(df[target_column])\n    plt.title(\"Target balance:\")\n    \n    # target distirbution\n    plt.subplot(2,1,2)\n    sns.stripplot(data= df, x=\"TARGET\",y=range(len(df)))\n    plt.title(\"Target distirbution\")\n\n\ndef nn_classifier(input_shape):\n    \"\"\"Build a nural network classifier\"\"\"\n    \n    from keras.models import Sequential\n    from keras.layers import InputLayer, Dense, Dropout\n    import keras\n    model = Sequential()\n    model.add(InputLayer(input_shape=input_shape))\n    model.add(Dense(32, activation='relu', kernel_initializer='random_normal'))\n    model.add(Dropout(0.3))\n    model.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n\n    model.compile(optimizer='adam', \n                  loss=keras.losses.BinaryCrossentropy(from_logits=False), metrics=[\"accuracy\"])\n    return model\n\nclass Data():\n    \"\"\"An high level data class.\n    \n    Supports getting train / test set and encoding\n    \"\"\"\n    \n    def __init__(self, train_set, test_set, target_column, unique_coulmn):\n        \"\"\"Init a data structure.\n        \n        Positional arguments:\n        train_set -- pandas dataframe for training\n        test_set -- pandas dataframe for testing\n        target_column -- target column of train set\n        unique_column -- the identifier coloumn of the data \"\"ID\"\"\n        \"\"\"\n        \n        self._train_set = train_set\n        self._test_set = test_set\n        self._target = target_column\n        self._id = unique_column\n    \n    def get_all(self):\n        \"\"\"Returns all the data without target column\"\"\"\n        \n        return self._train_set.drop([self._target], axis=1).append(self._test_set).reset_index(drop=True)\n    \n    def get_train(self):\n        \"\"\"Returns train set with target column\"\"\"\n        \n        return self._train_set.copy()\n    \n    def get_test(self):\n        \"\"\"Return test set\"\"\"\n        \n        return self._test_set.copy()\n    \n    def get_train_enc(self):\n        \"\"\"Returns train set mean encoded with target\"\"\"\n        \n        return mean_encode_categorical(self._train_set, self._target)\n    \n    def get_test_enc(self):\n        \"\"\"Return test set mean encoded\"\"\"\n        \n        return test_mean_encode_categorical(self._test_set, self._train_set, self._target)\n    \n    def get_all_enc(self):\n        \"\"\"Returns all data mean encoded\"\"\"\n        \n        return self.get_train_enc().drop([self._target], axis=1).append(self.get_test_enc()).reset_index(drop=True)\n    \n    def add_features(self, df_with_features):\n        \"\"\"Adds features from dataframe df to our data. df has to have the unique column\"\"\"\n        \n        self._train_set = self._train_set.join(df_with_features, on=self._id)\n        self._test_set = self._test_set.join(df_with_features, on=self._id)\n\ndef change_aggragate_column_names(aggs, prefix):\n    \"\"\"Changes aggragates columns names from tuples to strings.\"\"\"\n    \n    aggs.columns = pd.Index([prefix+ \"_\" + e[0] + \"_\" + e[1].upper() for e in aggs.columns.tolist()])\n    \ndef get_numeric_columns_aggragates(df, gp_column, prefix, to_agg=['min', 'max', 'mean', 'sum', 'var']):\n    \"\"\"Returns numerical columns aggragated features for dataframe grouped by gb_column\"\"\"\n    \n    numeric_columns = df.select_dtypes(np.number).columns # get numeric columns\n    if gp_column in numeric_columns:\n        numeric_columns = numeric_columns.drop([gp_column])\n    aggs = {column: to_agg for column in numeric_columns}\n    gp_aggs = df.groupby([gp_column]).agg(aggs)\n    change_aggragate_column_names(gp_aggs, prefix) # tidy up columns names\n    return gp_aggs\n\ndef get_bayesian_hp_space():\n    \"\"\"Returns a dictionary with \"\"\"\n    from hyperopt import hp\n    space = {\n    'boosting_type': hp.choice('boosting_type', \n                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n                                             {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n                                             {'boosting_type': 'goss', 'subsample': 1.0}]),\n    'num_leaves': hp.quniform('num_leaves', 20, 150, 1),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),\n    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n    'is_unbalance': hp.choice('is_unbalance', [True, False]),\n    }\n    return space\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"applications_org = pd.read_csv(\"/kaggle/input/home-credit-default-risk/application_train.csv\")\nbureau = pd.read_csv(\"/kaggle/input/home-credit-default-risk/bureau.csv\")\nprevious_app = pd.read_csv(\"/kaggle/input/home-credit-default-risk/previous_application.csv\")\napp_test = pd.read_csv(\"/kaggle/input/home-credit-default-risk/application_test.csv\")\nbureau_balance = pd.read_csv(\"/kaggle/input/home-credit-default-risk/bureau_balance.csv\")\ncredit_card_balance = pd.read_csv(\"/kaggle/input/home-credit-default-risk/credit_card_balance.csv\")\ninstallment_payments = pd.read_csv(\"/kaggle/input/home-credit-default-risk/installments_payments.csv\")\npos_cash = pd.read_csv(\"/kaggle/input/home-credit-default-risk/POS_CASH_balance.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explore_target(applications_org, \"TARGET\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print((applications_org[\"TARGET\"] == 1).sum() / len(applications_org)) # TARGET balance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merge train and test for feature generation"},{"metadata":{"trusted":true},"cell_type":"code","source":"applications = applications_org.drop([\"TARGET\"], axis=1)\napplications = applications.append(app_test).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utelizing applications for features"},{"metadata":{"trusted":true},"cell_type":"code","source":"applications[\"PAYMENT_RATE\"] = applications[\"AMT_ANNUITY\"] / applications[\"AMT_CREDIT\"]\napplications[\"RELATIVE_LOAN\"] = applications[\"AMT_CREDIT\"] / applications[\"AMT_INCOME_TOTAL\"]\napplications[\"PER_PERSON_INCOME\"] = applications[\"AMT_INCOME_TOTAL\"] / applications[\"CNT_FAM_MEMBERS\"]\napplications[\"ANNUITY_INCOME_PERC\"] = applications[\"AMT_ANNUITY\"] / applications[\"AMT_INCOME_TOTAL\"]\napplications[\"DAYS_EMPLOYED_PERC\"] = applications[\"DAYS_EMPLOYED\"] / applications[\"DAYS_BIRTH\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utelizing bureau table for features"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Create older loans status feature (num open / closed)\nbureau[\"OPEN_CREDITS\"] = bureau[\"CREDIT_ACTIVE\"].map({'Active': 1, 'Closed': 0})\ncredit_active_per_id = bureau.groupby([\"SK_ID_CURR\"])['OPEN_CREDITS'].sum() # sum open credits\nbureau[\"CLOSE_CREDITS\"] = bureau[\"CREDIT_ACTIVE\"].map({'Active': 0, 'Closed': 1})\ncredit_closed_per_id = bureau.groupby([\"SK_ID_CURR\"])['CLOSE_CREDITS'].sum() # sum closed credits\n# merge\napplications = applications.join(credit_active_per_id, on=\"SK_ID_CURR\")\napplications = applications.join(credit_closed_per_id, on=\"SK_ID_CURR\")\n\n\n## Create monthly balance features\nbb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\nbureau_balance_gp_agg = bureau_balance.groupby([\"SK_ID_BUREAU\"]).agg(bb_aggregations)\nbureau_balance_gp_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bureau_balance_gp_agg.columns.tolist()])\nbureau = bureau.join(bureau_balance_gp_agg, on=\"SK_ID_BUREAU\")\n\n## Create numeric aggs features\nnum_aggregations = {\n        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n        'DAYS_CREDIT_UPDATE': ['mean'],\n        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n        'AMT_ANNUITY': ['max', 'mean'],\n        'CNT_CREDIT_PROLONG': ['sum'],\n        'MONTHS_BALANCE_MIN': ['min'],\n        'MONTHS_BALANCE_MAX': ['max'],\n        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n    }\nbureau_gp_agg = bureau.groupby(\"SK_ID_CURR\").agg(num_aggregations)\nbureau_gp_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bureau_gp_agg.columns.tolist()])\napplications = applications.join(bureau_gp_agg, on=\"SK_ID_CURR\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utelizing previous applicaions table for features"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Create previous home credit loans status feature\nprevious_app[\"PREV_HC_APPROVED\"] = previous_app[\"NAME_CONTRACT_STATUS\"].map({'Approved': 1, 'Refused': 0, 'Canceled': 0 , 'Unused ofer': 0})\nprevious_app[\"PREV_HC_REFUSED\"] = previous_app[\"NAME_CONTRACT_STATUS\"].map({'Approved': 0, 'Refused': 1, 'Canceled': 0 , 'Unused ofer': 0})\nprevious_app[\"PREV_HC_CANCELED\"] = previous_app[\"NAME_CONTRACT_STATUS\"].map({'Approved': 0, 'Refused': 0, 'Canceled': 1 , 'Unused ofer': 0})\nprevious_app[\"PREV_HC_UNUSED\"] = previous_app[\"NAME_CONTRACT_STATUS\"].map({'Approved': 1, 'Refused': 0, 'Canceled': 0 , 'Unused ofer': 1})\n\nprev_hc_approved = previous_app.groupby([\"SK_ID_CURR\"])[\"PREV_HC_APPROVED\"].sum()\nprev_hc_refused = previous_app.groupby([\"SK_ID_CURR\"])[\"PREV_HC_REFUSED\"].sum()\nprev_hc_canceled = previous_app.groupby([\"SK_ID_CURR\"])[\"PREV_HC_CANCELED\"].sum()\nprev_hc_unused = previous_app.groupby([\"SK_ID_CURR\"])[\"PREV_HC_UNUSED\"].sum()\n\n#merge\napplications = applications.join(prev_hc_approved, on='SK_ID_CURR')\napplications = applications.join(prev_hc_refused, on='SK_ID_CURR')\napplications = applications.join(prev_hc_canceled, on='SK_ID_CURR')\napplications = applications.join(prev_hc_unused, on='SK_ID_CURR')\n    \n## Create previous hc agg features\nprevious_app['APP_CREDIT_PERC'] = previous_app['AMT_APPLICATION'] / previous_app['AMT_CREDIT']\nnum_aggregations = {\n        'AMT_ANNUITY': ['min', 'max', 'mean'],\n        'AMT_APPLICATION': ['min', 'max', 'mean'],\n        'AMT_CREDIT': ['min', 'max', 'mean'],\n        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n        'DAYS_DECISION': ['min', 'max', 'mean'],\n        'CNT_PAYMENT': ['mean', 'sum'],\n    }\nprev_app_aggs = previous_app.groupby([\"SK_ID_CURR\"]).agg(num_aggregations)\nprev_app_aggs.columns = pd.Index([\"PREV_\"+e[0] + \"_\" + e[1].upper() for e in prev_app_aggs.columns.tolist()])\napplications = applications.join(prev_app_aggs, on=\"SK_ID_CURR\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Create previous home credit loans rejection reason\nrej_reason = previous_app[\"CODE_REJECT_REASON\"].unique()\n\nfor reason in rej_reason:\n    # map that reason to ones and others to 0\n    reason_map = previous_app[\"CODE_REJECT_REASON\"].map(lambda res: 1 if (res == reason) else 0)\n    previous_app[\"PREV_REASON_REJ_{}_COUNT\".format(reason)] = reason_map\n    # sum that reason apearance through ids\n    reason_sum = previous_app.groupby(['SK_ID_CURR'])[\"PREV_REASON_REJ_{}_COUNT\".format(reason)].sum()\n    #merge\n    applications = applications.join(reason_sum, on='SK_ID_CURR')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utelizing credit card balance for features"},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_card_balance = credit_card_balance.drop([\"SK_ID_PREV\"], axis=1)\ncc_aggs = get_numeric_columns_aggragates(credit_card_balance, \"SK_ID_CURR\", \"CC\")\napplications = applications.join(cc_aggs, on=\"SK_ID_CURR\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get depth 2 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"ip_prev_sk_id_aggs = get_numeric_columns_aggragates(installment_payments[[\"SK_ID_PREV\", \"DAYS_INSTALMENT\",  \"AMT_PAYMENT\", \"AMT_INSTALMENT\"]]\n                                                   , \"SK_ID_PREV\", \"IP_GP_PREV\")\ninstallment_payments = installment_payments.join(ip_prev_sk_id_aggs, on=\"SK_ID_CURR\")\n\npos_cash_prev_sk_id_aggs = get_numeric_columns_aggragates(pos_cash[[\"SK_ID_PREV\", \"MONTHS_BALANCE\", \"CNT_INSTALMENT\", \"CNT_INSTALMENT_FUTURE\"]], \"SK_ID_PREV\", \"POS_CASH_GP_PREV\")\npos_cash = pos_cash.join(pos_cash_prev_sk_id_aggs, on=\"SK_ID_PREV\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utelizing installment payment for features"},{"metadata":{"trusted":true},"cell_type":"code","source":"installment_payments = installment_payments.drop([\"SK_ID_PREV\"], axis=1)\nip_aggs = get_numeric_columns_aggragates(installment_payments, \"SK_ID_CURR\", \"IP\")\napplications = applications.join(ip_aggs, on=\"SK_ID_CURR\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utelize POS CASH for features"},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_cash = pos_cash.drop([\"SK_ID_PREV\"], axis=1)\npos_cash_aggs = get_numeric_columns_aggragates(pos_cash, \"SK_ID_CURR\", \"POS\")\napplications = applications.join(pos_cash_aggs, on=\"SK_ID_CURR\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"applications = applications.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean encode train applications with expanding mean\ntrain_app = applications[:len(applications_org)]\ntrain_app[\"TARGET\"] = applications_org[\"TARGET\"]\n#train_app_enc = mean_encode_categorical(train_app, \"TARGET\")\n#train_app_enc = train_app_enc.drop([\"TARGET\"], axis=1)\n\n# mean encode test applications with train whole mean\ntest_app = applications[len(applications_org):]\ntest_app_enc = test_mean_encode_categorical(test_app, train_app, \"TARGET\")\n\n# append train and test again\n#applications_enc = train_app_enc.append(test_app_enc).reset_index()\n#applications_enc = applications_enc.drop([\"index\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#un_corr_feats = get_uncorrolated_features(applications_enc, 0.9)\nun_corr_feats = pd.read_csv(\"/kaggle/input/uncorrelated-features/uncorr_feats.csv\")[\"0\"].tolist() # load from previous sessions\nprint(len(un_corr_feats))\n#print_feature_correlation(applications_enc[un_corr_feats])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pd.DataFrame(un_corr_feats).to_csv(\"/kaggle/working/uncorr_feats.csv\") # saving feats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lgbm_eval(train_app[un_corr_feats], target_col=\"TARGET\", n_splits=1, test_size=0.2, verbose=True, n_rounds=6000))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Use bayesian search for hyper parameter optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import hyperopt as hp\nfrom hyperopt import tpe, Trials, fmin\nfrom hyperopt import STATUS_OK\n\nspace = get_bayesian_hp_space()\ntrials = Trials()\n\ndef objective(params): \n    subsample = params['boosting_type'].get('subsample', 1.0) # make subsample a top-level key\n    params['subsample'] = subsample\n    params['boosting_type'] = params['boosting_type']['boosting_type'] # make boosting type a top-level key\n    params['early_stopping_round'] = 100\n    params['metric'] = 'auc'\n    params['objective'] = 'binary'\n    \n    for param in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n        params[param] = int(params[param]) # make sure supposed-to-be-int params are int\n    \n    return {'loss': 1-lgbm_eval(train_app[un_corr_feats].sample(100000), target_col=\"TARGET\", n_splits=2, test_size=0.2,\n                                  verbose=False, n_rounds=2000, lgb_params=params), 'hyperparameters': params, 'status': STATUS_OK}\n\n\nbest_params = fmin(fn=objective, algo=tpe.suggest, space=space, trials=trials, max_evals=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"#really_un_corr_feats = get_uncorrolated_features(applications_enc, 0.7)\nreally_un_corr_feats = pd.read_csv(\"/kaggle/input/uncorrelated-features/really_uncorr_feats.csv\")[\"0\"].tolist() # load from previous sessions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pd.DataFrame(really_un_corr_feats).to_csv(\"/kaggle/working/really_uncorr_feats.csv\") # saving feats#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#U$U$mreally un corr feats\n# one hot encode categorical features\napp_with_dummies = pd.get_dummies(applications[really_un_corr_feats], prefix_sep='_', drop_first=True)\n# get only train set\ntrain_with_dummies = app_with_dummies[:len(applications_org)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## preproccess data for nn\nnorm_train = normalize_dataframe(train_with_dummies) \nnorm_train = replace_numeric_columns_nulls(norm_train) # fill nulls with avarege","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn = nn_classifier((len(norm_train.columns), )) # build classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train set\nX_train = norm_train[:-50000]\nY_train = applications_org[\"TARGET\"][:-50000]\n# test set\n\nX_test= norm_train[-50000:]\nY_test = applications_org[\"TARGET\"][-50000:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = nn.fit(X_train, Y_train, batch_size=128, epochs=25, validation_data=(X_test,Y_test), shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nprint(roc_auc_score(Y_test, nn.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get nn classifier features for lgbm"},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize entire dataframe for nn digestion\nnorm_app = normalize_dataframe(app_with_dummies)\nnorm_app = replace_numeric_columns_nulls(norm_app)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_app[\"NN_PREDICTION\"] = nn.predict(norm_app[:len(applications_org)])\ntest_app_enc[\"NN_PREDICTION\"] = nn.predict(norm_app[len(applications_org):])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"un_corr_feats.append(\"NN_PREDICTION\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.enable()\ndel app_with_dummies\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"copied_params = {\n    'thread':4,\n    'n_estimators':10000,\n    'learning_rate':0.03,\n    'num_leaves':34,\n    'colsample_bytree':0.9497036,\n    'subsample':0.8715623,\n    'max_depth':8,\n    'reg_alpha':0.041545473,\n    'reg_lambda':0.0735294,\n    'min_split_gain':0.0222415,\n    'min_child_weight':39.3259775,\n    'metric': 'auc',\n    'objective': 'binary',\n    'early_stopping_rounds':100\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"best_params['boosting_type'] = 'goss'\nfor param in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n        best_params[param] = int(best_params[param])\nbest_params['is_unbalance'] = True\nbest_params['early_stopping_rounds'] = 100\nbest_params['verbose'] = 0\nbest_params['metric'] = 'auc'\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, model = lgbm_eval(train_app[un_corr_feats], target_col=\"TARGET\", n_splits=1, test_size=0.1, verbose=True, get_model=True, n_rounds=6000, lgb_params=copied_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_feats = un_corr_feats.copy()\ntest_feats.remove(\"TARGET\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/home-credit-default-risk/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[\"TARGET\"] = model.predict(test_app_enc[test_feats]) # lgbm predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"/kaggle/working/submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}